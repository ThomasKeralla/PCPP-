{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Menlo-Regular;\f2\froman\fcharset0 Times-Roman;
}
{\colortbl;\red255\green255\blue255;\red47\green180\blue29;\red255\green255\blue255;\red159\green160\blue28;
\red0\green0\blue0;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c20238\c73898\c14947;\csgray\c100000;\cssrgb\c68468\c68012\c14208;
\csgray\c0;\cssrgb\c0\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b\fs28 \cf0 Parallel Programming - Exercise 9\

\fs24 Thomas Keralla H\'f8pfner-Dahl
\fs28 \
\

\b0\fs24 9.2\
\
1)\
\
public StmHistogram(int span) \{\
      counts = new TxnInteger[span];\
\
      for (int i = 0; i < span; i++)\
          counts[i] = newTxnInteger(0);\
  \}\
\
  public void increment(int bin) \{\
      counts[bin].atomicIncrementAndGet(1);\
  \}\
\
  public int getCount(int bin) \{\
     return counts[bin].atomicGet();\
  \}\
\
  public int getSpan() \{\
    return counts.length;\
\
2)\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\fs22 \cf2 \cb3 \CocoaLigature0 thd@Thomasadmins-MacBook-Pro\cf4  ~/OneDrive/MSc/Parallel programming/Handins/hi9-not.done/stm\cf5 \cb3 \
$ java -cp ~/lib/multiverse-core-0.7.0.jar:. TestStmHistogram \
Nov 25, 2018 10:29:08 PM org.multiverse.api.GlobalStmInstance <clinit>\
INFO: Initializing GlobalStmInstance using factoryMethod 'org.multiverse.stms.gamma.GammaStm.createFast'.\
Nov 25, 2018 10:29:08 PM org.multiverse.api.GlobalStmInstance <clinit>\
INFO: Successfully initialized GlobalStmInstance using factoryMethod 'org.multiverse.stms.gamma.GammaStm.createFast'.\
**********   0:         2\
   1:    283146\
   2:    790986\
   3:    988651\
   4:    810386\
   5:    524171\
   6:    296702\
   7:    155475\
   8:     78002\
   9:     38069\
  10:     18232\
  11:      8656\
  12:      4055\
  13:      1886\
  14:       865\
  15:       400\
  16:       179\
  17:        79\
  18:        35\
  19:        14\
  20:         7\
  21:         2\
  22:         0\
  23:         0\
  24:         0\
  25:         0\
  26:         0\
  27:         0\
  28:         0\
  29:         0\
        4000000
\f0\fs24 \cf0 \cb1 \CocoaLigature1 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \
3)\
public int[] getBins() \{\
      int[] bins = new int[counts.length];\
      for (int i = 0; i < counts.length; i++) \
          bins[i] = counts[i].atomicGet();\
\
      return bins;\
  \}\
\
4)\
  public int getAndClear(int bin) \{\
      return counts[bin].atomicGetAndSet(0);\
  \}\
\
5)\
public void transferBins(Histogram hist) \{\
      for (int i=0; i<hist.getSpan(); i++) \{\
        final int n = i;\
          atomic(() -> counts[n].increment(hist.getAndClear(n)));\
      \}\
  \}\
\
6)\
for (int i = 0; i < 200; i++) \{\
        try \{\
            total.transferBins(histogram);\
            Thread.sleep(30);\
        \} catch (InterruptedException e) \{\
            e.printStackTrace();\
        \}\
    \}\
\
    System.out.println("Histogram dump");\
    dump(histogram);\
    System.out.println("Total Histogram dump");\
    dump(total);\
\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\fs22 \cf5 \cb3 \CocoaLigature0 **********Histogram dump\
   0:         0\
   1:         0\
   2:         0\
   3:         0\
   4:         0\
   5:         0\
   6:         0\
   7:         0\
   8:         0\
   9:         0\
  10:         0\
  11:         0\
  12:         0\
  13:         0\
  14:         0\
  15:         0\
  16:         0\
  17:         0\
  18:         0\
  19:         0\
  20:         0\
  21:         0\
  22:         0\
  23:         0\
  24:         0\
  25:         0\
  26:         0\
  27:         0\
  28:         0\
  29:         0\
              0\
Total Histogram dump\
   0:         2\
   1:    283146\
   2:    790986\
   3:    988651\
   4:    810386\
   5:    524171\
   6:    296702\
   7:    155475\
   8:     78002\
   9:     38069\
  10:     18232\
  11:      8656\
  12:      4055\
  13:      1886\
  14:       865\
  15:       400\
  16:       179\
  17:        79\
  18:        35\
  19:        14\
  20:         7\
  21:         2\
  22:         0\
  23:         0\
  24:         0\
  25:         0\
  26:         0\
  27:         0\
  28:         0\
  29:         0\
        4000000\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \cb1 \CocoaLigature1 \
Yes that is the case\
\
7)\
I would expect total to replicate itself as it would just clear its bin holding the value locally to then put it back in, seeing no changes when dumping it. It seems like that is the case when I try it out. \
\

\b 9.3\
\

\b0 1)\
  public V get(K k) \{\
    Holder<V> holder = new Holder<>();\
    ItemNode<K, V> node = atomic(() -> \{\
        TxnRef<ItemNode<K, V>>[] refs = buckets.atomicGet();\
        int hash = getHash(k) % refs.length;\
        return refs[hash].atomicGet();\
    \});\
    if(ItemNode.search(node, k, holder)) \{\
        return holder.get();\
      \}\
      return null;\
  \}\
\
2)\
public void forEach(Consumer<K, V> consumer) \{\
      final TxnRef<ItemNode<K,V>>[] bs = buckets.atomicGet();\
      for (int i = 0; i < bs.length; i++) \{\
          ItemNode<K,V> node = bs[i].atomicGet();\
          while (node != null) \{\
              consumer.accept(node.k, node.v);\
              node = node.next;\
          \}\
      \}\
  \}\
\
3)\
  // Put v at key k, or update if already present.\
  public V put(K k, V v) \{\
      Holder<V> holder = new Holder<>();\
      final TxnRef<ItemNode<K,V>>[] bs = buckets.atomicGet();\
      int hash = getHash(k);\
      int index = hash % bs.length;\
      ItemNode<K,V> node = bs[index].atomicGet();\
\
      if (ItemNode.search(node, k, holder)) \{\
          V val = holder.get();\
          holder.set(v);\
          return val;\
      \} else \{\
          node = new ItemNode<>(k,v, node);\
          cachedSize.atomicIncrementAndGet(1);\
          return null;\
      \}\
  \}\
\
  // Put v at key k only if absent.\
  public V putIfAbsent(K k, V v) \{\
      Holder<V> holder = new Holder<>();\
      final TxnRef<ItemNode<K,V>>[] bs = buckets.atomicGet();\
      int index = getHash(k) % bs.length;\
\
      ItemNode<K,V> node = bs[index].atomicGet();\
\
      if(!ItemNode.search(node, k, holder))\{\
          node = new ItemNode<>(k,v, node);\
          cachedSize.atomicIncrementAndGet(1);\
          return node.v;\
      \}\
      return null;\
  \}\
\
  // Remove and return the value at key k if any, else return null\
  public V remove(K k) \{\
      Holder<V> holder = new Holder<>();\
      final TxnRef<ItemNode<K,V>>[] bs = buckets.atomicGet();\
      int index = getHash(k) % bs.length;\
      ItemNode<K,V> node = bs[index].atomicGet();\
      ItemNode<K,V> deleted = ItemNode.delete(node, k, holder);\
\
      if (deleted != null) \{\
          cachedSize.atomicIncrementAndGet(-1);\
          return deleted.v;\
      \}\
      return null;\
  \}\
\
4)\
public int size() \{\
    return cachedSize.atomicGet();\
  \}\
\
5)\
As already described in the exercise the reallocate buckets is, unless a proper protocol is given, not very optimised for using transactions. This is because transactions works best in a optimistic environment with short transaction. The reallocateBuckets needs to lock the entire map in order to avoid a retry() call if the map has changed state in the meantime, which very likely, otherwise all the resources used computing will be lost when the retry is called. \
The opposed solution with having one shared transactional field that has to be checked/read at chokepoints and using  await(
\f2\fs26\fsmilli13333 \cf6 \expnd0\expndtw0\kerning0
newBuckets 
\f0\fs24 \cf0 \kerning1\expnd0\expndtw0 ) as blocking seems like a good protocol. All transactions started would have to roll back and restart when the blocking is lifted and the newBuckets value is back to null, but this seems like a fair price to pay for avoiding the potential infinite loop for reallocating buckets and at very least lost of wasted computing. \
The atomic get() method for reading the value is inexpensive and even though all threads would have to call this method a lot more frequently the wait-time for other threads making the same call seems justifiable as it is unlikely that it would result in many calls to retry().  \
}